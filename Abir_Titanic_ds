import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

df_train=pd.read_csv('train.csv')
df_test=pd.read_csv('test.csv')

df_train.head()

df_train.info()

## AGE AND CABIN HAS LOTS OF NULL VALUES IN TEST DATASET

df_train.isnull().sum()

df_test.head()

df_test.info()

## AGE AND CABIN HAS LOTS OF NULL VALUES IN TRAIN DATASET 

df_test.isnull().sum()

df_train.head()

survived=df_train[df_train["Survived"]==1]
not_survived=df_train[df_train["Survived"]==0]

print("Survived: %i (%.1f%%)"%(len(survived),float(len(survived))/len(df_train)*100.0))

print("Not Survived: %i (%.1f%%)"%(len(not_survived),float(len(not_survived))/len(df_train)*100.0))

df_train.Pclass.value_counts()

df_train.groupby('Pclass').Survived.value_counts()

df_train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean()

## Higher the Pclass has, higher the chances of survival

sns.barplot(x='Pclass',y='Survived',data=df_train)

df_train.Sex.value_counts()

df_train.groupby('Sex').Survived.value_counts()

df_train[['Sex','Survived']].groupby(['Sex'],as_index=False).mean()

## The number of female survived is more in numbers than men

sns.barplot(x='Sex',y='Survived',data=df_train)

df_train.SibSp.value_counts()

df_train.groupby('SibSp').Survived.value_counts()

df_train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean()

sns.barplot(x='SibSp',y='Survived',data=df_train)

## Family with 1 sibling or spouse has higher survival chance and Family with more than 4 Siblings or spouse has no survival chances

df_train.Parch.value_counts()

df_train.groupby('Parch').Survived.value_counts()

df_train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean()

sns.barplot(x='Parch',y='Survived',data=df_train)

df_train.Embarked.value_counts()

df_train.groupby('Embarked').Survived.value_counts()

df_train[['Embarked','Survived']].groupby(['Embarked'],as_index=False).mean()

sns.barplot(x='Embarked',y='Survived',data=df_train)

m=pd.crosstab(df_train['Pclass'],df_train['Sex'])
print(m)

m.div(m.sum(1).astype(float),axis=0).plot(kind="bar",stacked=True)
plt.xlabel('Pclass')
plt.ylabel('Percentage')

sns.factorplot('Sex','Survived',hue='Pclass',size=4,aspect=2,data=df_train)

sns.factorplot(x='Pclass',y='Survived',hue='Sex',col='Embarked',data=df_train)

fig=plt.figure(figsize=(15,5))
ax1=fig.add_subplot(131)
ax2=fig.add_subplot(132)
ax3=fig.add_subplot(133)
sns.violinplot(x="Embarked",y="Age",hue="Survived",data=df_train,split=True,ax=ax1)
sns.violinplot(x="Pclass",y="Age",hue="Survived",data=df_train,split=True,ax=ax2)
sns.violinplot(x="Sex",y="Age",hue="Survived",data=df_train,split=True,ax=ax3)

total_survived= df_train[df_train['Survived']==1]
total_not_survived= df_train[df_train['Survived']==0]

survived_m= df_train[(df_train['Survived']==1) & (df_train["Sex"]=="male")]
survived_f= df_train[(df_train['Survived']==1) & (df_train["Sex"]=="female")]

not_survived_m= df_train[(df_train["Survived"]==0) & (df_train["Sex"]=="male")]
not_survived_f= df_train[(df_train["Survived"]==0) & (df_train["Sex"]=="female")]

plt.figure(figsize=[15,5])
plt.subplot(111)
sns.distplot(total_survived["Age"].dropna().values, bins=range(0, 81, 1), kde=False, color='blue' )
sns.distplot(total_not_survived["Age"].dropna().values, bins=range(0, 81, 1), kde=False, color='red', axlabel="Age")

plt.figure(figsize=[15,5])
plt.subplot(121)
sns.distplot(survived_f["Age"].dropna().values, bins=range(0,81,1),kde=False, color='blue')
sns.distplot(not_survived_f["Age"].dropna().values,bins=range(0,81,1),kde=False,color='red',axlabel="Female Age")

plt.subplot(122)
sns.distplot(survived_m["Age"].dropna().values, bins=range(0,81,1),kde=False,color='blue')
sns.distplot(not_survived_m["Age"].dropna().values, bins=range(0,81,1),kde=False, color='red', axlabel="Male Age")



## Correlating Features

plt.figure(figsize=(15,6))
sns.heatmap(df_train.drop('PassengerId',axis=1).corr(),vmax=0.6,square=True,annot=True)

comb=[df_train,df_test]

for df in comb:
  df['Title']=df.Name.str.extract('([A-Za-z]+)\.')

df_train.head()

pd.crosstab(df_train['Title'],df_train['Sex'])

for df in comb:
    df['Title']=df['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Lady','Major','Rev','Sir'],'Other')
    df['Title']=df['Title'].replace('Mlle','Miss')
    df['Title']=df['Title'].replace('Ms','Miss')
    df['Title']=df['Title'].replace('Mme','Mrs')
df_train[['Title','Survived']].groupby(['Title'],as_index=False).mean()

title_map={"Master":1,"Miss":2,"Mr":3,"Mrs":4,"Others":5}

for df in comb:
    df['Title']=df['Title'].map(title_map)
    df['Title']=df['Title'].fillna(0)

df_train.head()

for df in comb:
    df['Sex']=df['Sex'].map({'female':1,"male":0}).astype(int)

df_train.head()

df_train.Embarked.unique()

df_train.Embarked.value_counts()

for df in comb:
    df['Embarked']=df['Embarked'].fillna('S')

df_train.head()

df_train.Embarked.unique()

for df in comb:
    df['Embarked']=df['Embarked'].map({'S':0,'C':1,'Q':2}).astype(int)

df_train.head()

df_train.Age.unique()

for df in comb:
    agemean=df['Age'].mean()
    agestd=df['Age'].std()
    agenull=df['Age'].isnull().sum()
    
    agenull_rand=np.random.randint(agemean-agestd,agemean+agestd,size=agenull)
    df['Age'][np.isnan(df['Age'])]=agenull_rand
    df['Age']=df['Age'].astype(int)
   
df_train['AgeRange']=pd.cut(df_train['Age'], 5)

print(df_train[['AgeRange','Survived']].groupby(['AgeRange'],as_index=False).mean())

df_train.head()

for df in comb:
    df.loc[df['Age'] <=16,'Age']=0
    df.loc[(df['Age']>16) & (df['Age']<=32),'Age']=1
    df.loc[(df['Age']>32) & (df['Age']<=48),'Age']=2
    df.loc[(df['Age']>48) & (df['Age']<=64),'Age']=3
    df.loc[df['Age']>64,'Age']=4

df_train.head()

for df in comb:
    df['Fare']=df['Fare'].fillna(df_train['Fare'].median())

df_train['FareRange']=pd.qcut(df_train['Fare'],4)
print(df_train[['FareRange','Survived']].groupby(['FareRange'],as_index=False).mean())

df_train.head()

for df in comb:
    df.loc[df['Fare']<=7.91,'Fare']=0
    df.loc[(df['Fare']>7.91) & (df['Fare']<=14.454),'Fare']=1
    df.loc[(df['Fare']>14.454) & (df['Fare']<=31.0),'Fare']=2
    df.loc[(df['Fare']>31.0) & (df['Fare']<=512.329),'Fare']=3
    df['Fare']=df['Fare'].astype(int)

df_train.head()

for df in comb:
    df['Family']=df['SibSp']+df['Parch']+1
    
print(df_train[['Family','Survived']].groupby(['Family'],as_index=False).mean())

for df in comb:
    df['Alone']=0
    df.loc[df['Family']==1,'Alone']=1
    
print(df_train[['Alone','Survived']].groupby(['Alone'],as_index=False).mean())

df_train.head()

df_test.head()

feature_drop=['Name','SibSp','Parch','Ticket','Cabin','Family']
df_train=df_train.drop(feature_drop,axis=1)
df_test=df_test.drop(feature_drop,axis=1)
df_train=df_train.drop(['PassengerId','AgeRange','FareRange'],axis=1)

df_train.head()

df_test.head()

x_train=df_train.drop('Survived',axis=1)
y_train=df_train['Survived']
x_test=df_test.drop("PassengerId",axis=1).copy()
x_train.shape,y_train.shape,x_test.shape

## Classification and Accuracy

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC, LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import Perceptron
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier

log=LogisticRegression()
log.fit(x_train,y_train)
pred_log=log.predict(x_test)
acc=round(log.score(x_train,y_train)*100,2)
print(str(acc)+ ' % ')

svm=SVC()
svm.fit(x_train,y_train)
pred_svm=svm.predict(x_test)
acc_svm=round(svm.score(x_train,y_train)*100,2)
print(str(acc_svm)+ ' % ')

lin=LinearSVC()
lin.fit(x_train,y_train)
pred_lin=lin.predict(x_test)
acc_lin=round(lin.score(x_train,y_train)*100,2)
print(str(acc_lin)+ ' % ')

knn=KNeighborsClassifier(n_neighbors=3)
knn.fit(x_train,y_train)
pred_knn=knn.predict(x_test)
acc_knn=round(knn.score(x_train,y_train)*100,2)
print(str(acc_knn)+ ' % ')

dec=DecisionTreeClassifier()
dec.fit(x_train,y_train)
pred_dec=dec.predict(x_test)
acc_dec=round(dec.score(x_train,y_train)*100,2)
print(str(acc_dec)+ ' % ')

rf=RandomForestClassifier(n_estimators=100)
rf.fit(x_train,y_train)
pred_rf=rf.predict(x_test)
acc_rf=round(rf.score(x_train,y_train)*100,2)
print(str(acc_rf)+ ' % ')

gnb=GaussianNB()
gnb.fit(x_train,y_train)
pred_gnb=gnb.predict(x_test)
acc_gnb=round(gnb.score(x_train,y_train)*100,2)
print(str(acc_gnb)+ ' % ')

## Confusion Matrix

from sklearn.metrics import confusion_matrix
import itertools

rf1=RandomForestClassifier(n_estimators=100)
rf1.fit(x_train,y_train)
pred_rf1=rf1.predict(x_train)
acc_rf1=round(rf1.score(x_train,y_train)*100,2)
print("Accuracy: %i %% \n"%acc_rf1)

x0=['Survived','Not Survived']

con_mat=confusion_matrix(y_train,pred_rf1)
np.set_printoptions(precision=2)

print('Confusion Matrix in Numbers')
print(con_mat)
print('')

con_matrix_percent=con_mat.astype('float')/con_mat.sum(axis=1)[:,np.newaxis]

print('Confusion Matrix in Percentage')
print(con_matrix_percent)
print('')

true_class_names=['True Survived','True Not Survived']
predicted_class_names=['Predicted Survived','Predicted Not Survived']

df_con_mat=pd.DataFrame(con_mat,index=true_class_names,columns=predicted_class_names)
df_con_mat_percent=pd.DataFrame(con_matrix_percent,index=true_class_names,columns=predicted_class_names)

plt.figure(figsize=(15,5))

plt.subplot(121)
sns.heatmap(df_con_mat,annot=True,fmt='d')

plt.subplot(121)
sns.heatmap(df_con_mat_percent,annot=True)

## Model Comparison

models=pd.DataFrame({
    'Model':['Logistic Regression','Support Vector Machine','Linear SVC','KNN','Decision Tree','Random Forest','Naive Bayes'],

    'Score':[acc,acc_svm,acc_lin,acc_knn,acc_dec,acc_rf,acc_gnb]

})
models.sort_values(by='Score',ascending=False)

df_test.head()

submission=pd.DataFrame({
        "PassengerId": df_test["PassengerId"], 
           "Survived":pred_rf
})

# submission.to_csv('submission.csv',index=False)

